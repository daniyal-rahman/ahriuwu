# DreamerV4 Architecture Review - Scale 2 Implementation

**Date:** 2025-12-27
**Files Reviewed:**
- `/Users/danirahman/Repos/ahriuwu/src/ahriuwu/models/dynamics.py`
- `/Users/danirahman/Repos/ahriuwu/src/ahriuwu/models/tokenizer.py`
- `/Users/danirahman/Repos/ahriuwu/src/ahriuwu/models/diffusion.py`

**Reference:** DreamerV4 Paper - "Training Agents Inside of Scalable World Models" (Hafner et al., 2025)

---

## Executive Summary

This review compares the implementation against the DreamerV4 paper specifications. The implementation represents a **simplified/scaled-down version** of DreamerV4 suitable for initial development and experimentation. Several key architectural differences exist, categorized as intentional simplifications, missing features, or potential bugs.

---

## 1. Tokenizer Architecture (Paper Section 3.1)

### Paper Specification

| Component | Paper Specification |
|-----------|-------------------|
| Architecture | Block-causal TRANSFORMER encoder/decoder |
| Bottleneck | Tanh activation, (N_b=512) x (D_b=16) reshaped to (N_z=256) x 32 |
| Training | MAE with dropout p ~ U(0, 0.9) |
| Loss | MSE + 0.2 * LPIPS |
| Output | Continuous latent representations |

### Implementation Status

| Component | Implementation | Match |
|-----------|---------------|-------|
| Architecture | **CNN-based** ResNet autoencoder | NO |
| Bottleneck | Linear projection, **no tanh activation** | NO |
| MAE Training | **Not implemented** | NO |
| Spatial Output | 16x16 latent grid (256 tokens) | YES |

### Detailed Analysis

#### 1.1 Encoder/Decoder Architecture

**Paper (Section 3.1):**
> "We use the efficient transformer architecture described later. Each time step consists of patch tokens of the current image and learned latent tokens."

**Implementation (`tokenizer.py` lines 67-99):**
```python
class Encoder(nn.Module):
    """CNN encoder: 256x256x3 -> 16x16xlatent_dim"""
    def __init__(self, latent_dim: int = 256, base_channels: int = 64):
        # ...
        self.stem = nn.Sequential(
            nn.Conv2d(3, base_channels, 7, stride=1, padding=3),
            # ...
        )
        # Downsampling stages: 256 -> 128 -> 64 -> 32 -> 16
        self.stage1 = ResBlock(base_channels, base_channels * 2, downsample=True)
        # ...
```

**Assessment:** INTENTIONAL SIMPLIFICATION

The implementation uses a CNN-based architecture instead of the block-causal transformer specified in the paper. This is a significant architectural difference but is likely intentional for the MVP phase:
- CNNs are simpler to implement and debug
- Lower computational requirements for initial experiments
- Faster training for validation purposes

**Recommendation:** For scale 2+, consider migrating to transformer-based tokenizer to match paper architecture and enable temporal compression benefits described in Section 3.1.

#### 1.2 Tanh Bottleneck

**Paper (Section 3.1):**
> "the representations are read out of the latent tokens using a linear projection to a smaller channel dimension followed by a tanh activation"

**Implementation (`tokenizer.py` lines 162-168):**
```python
def encode(self, x: torch.Tensor) -> torch.Tensor:
    """Encode image to latent representation."""
    return self.encoder(x)  # No tanh activation
```

**Assessment:** MISSING FEATURE - SHOULD BE ADDED

The tanh bottleneck serves important purposes:
1. Bounds the latent space to [-1, 1]
2. Provides regularization
3. Makes diffusion noise scheduling more predictable
4. Referenced in paper: "(N_b=512) x (D_b=16) reshaped to (N_z=256) x 32"

**Recommendation:** Add tanh activation after encoder output projection. This is a simple but important addition.

#### 1.3 MAE Training

**Paper (Section 3.1):**
> "We drop out input patches to the encoder to improve its representations using masked autoencoding. The dropout probability is randomized across images as p ~ U(0, 0.9)."

**Implementation:** Not present in tokenizer.py

**Assessment:** MISSING FEATURE - SHOULD BE ADDED

MAE training is explicitly stated to "improve the spatial consistency of videos generated by the dynamics model." This is important for downstream dynamics model quality.

**Recommendation:** Implement MAE training with:
- Patchified input (already compatible with 16x16 grid)
- Random dropout p ~ U(0, 0.9) per image
- Learned mask embedding

---

## 2. Dynamics Model (Paper Section 3.2, 3.4)

### Paper Specification

| Component | Paper Specification |
|-----------|-------------------|
| Architecture | 2D transformer with time and space dimensions |
| Normalization | Pre-layer RMSNorm |
| Position Encoding | RoPE (Rotary Position Embeddings) |
| FFN | SwiGLU |
| Attention Features | QKNorm, attention logit soft capping |
| Temporal Attention | Every 4 layers |
| Attention Type | GQA (Grouped Query Attention) |
| Special Tokens | Register tokens for temporal consistency |
| Size | 1.6B parameters (full scale) |

### Implementation Status

| Component | Implementation | Match |
|-----------|---------------|-------|
| 2D Transformer | Factorized spatial/temporal attention | YES |
| Pre-layer RMSNorm | Implemented | YES |
| RoPE | **NOT implemented** (learned positional embeddings) | NO |
| SwiGLU | Implemented | YES |
| QKNorm | **NOT implemented** | NO |
| Attention Soft Capping | **NOT implemented** | NO |
| Temporal Every 4 Layers | Implemented | YES |
| GQA | **NOT implemented** (standard MHA) | NO |
| Register Tokens | **NOT implemented** | NO |
| AdaLN | Implemented | YES |

### Detailed Analysis

#### 2.1 Position Embeddings (CRITICAL)

**Paper (Section 3.4):**
> "We start from a standard transformer with pre-layer RMSNorm, RoPE, and SwiGLU."

**Implementation (`dynamics.py` lines 314-319):**
```python
# Positional embeddings
self.spatial_pos = nn.Parameter(
    torch.randn(1, 1, self.spatial_tokens, model_dim) * 0.02
)
self.temporal_pos = nn.Parameter(
    torch.randn(1, max_seq_len, 1, model_dim) * 0.02
)
```

**Assessment:** MISSING FEATURE - SHOULD BE ADDED

The implementation uses **learned positional embeddings** instead of RoPE. This is a significant difference:

| Aspect | Learned Embeddings | RoPE |
|--------|-------------------|------|
| Length Generalization | Poor | Good |
| Relative Position | No | Yes |
| Memory | O(max_len * dim) | O(dim) |
| Paper Compliance | No | Yes |

The paper explicitly uses RoPE for better length generalization, which is critical for:
- Generating videos longer than training context
- Temporal consistency in long rollouts
- The "length generalization to arbitrary generation lengths" mentioned in Section 3.4

**Recommendation:** Implement RoPE for both spatial and temporal dimensions. This is a moderate complexity change with significant benefits.

#### 2.2 QKNorm and Attention Soft Capping

**Paper (Section 3.4):**
> "We employ QKNorm and attention logit soft capping to increase training stability."

**Implementation (`dynamics.py` lines 104-107):**
```python
# Scaled dot-product attention
attn = (q @ k.transpose(-2, -1)) * self.scale
attn = F.softmax(attn, dim=-1)
attn = self.dropout(attn)
```

**Assessment:** MISSING FEATURE - SHOULD BE ADDED

Neither QKNorm nor soft capping is implemented:

**QKNorm:** Normalizes Q and K before attention computation:
```python
# Missing implementation:
q = F.normalize(q, dim=-1)
k = F.normalize(k, dim=-1)
```

**Soft Capping:** Limits attention logits to prevent instability:
```python
# Missing implementation:
attn = attn / cap_value
attn = torch.tanh(attn) * cap_value
```

**Recommendation:** Add both features for training stability at scale. Referenced in Gemma 2 paper (reference 37 in DreamerV4).

#### 2.3 Grouped Query Attention (GQA)

**Paper (Section 3.4):**
> "Third, we apply GQA to all attention layers in the dynamics, where multiple query heads attend to the same key-value heads to reduce the KV cache size further."

**Implementation (`dynamics.py` lines 66-84):**
```python
class SpatialAttention(nn.Module):
    # ...
    self.qkv = nn.Linear(dim, inner_dim * 3, bias=False)  # Standard MHA
```

**Assessment:** INTENTIONAL SIMPLIFICATION (for scale)

GQA is primarily an efficiency optimization:
- Reduces KV cache size
- Important for inference speed at scale
- Less critical for small model experiments

**Recommendation:** Add GQA when scaling up. For MVP/small scale, standard MHA is acceptable.

#### 2.4 Register Tokens

**Paper (Section 3.2):**
> "concatenated with S_r learned register tokens"

**Paper (Section 3.4 / Table 2):**
> "Register tokens... do not improve FVD measurably, [but] we qualitatively notice that they improve temporal consistency."

**Implementation:** Not present

**Assessment:** MISSING FEATURE - LOW PRIORITY

Register tokens provide qualitative improvements in temporal consistency but don't significantly affect metrics. Lower priority than RoPE, QKNorm.

**Recommendation:** Consider adding after core architecture is validated.

#### 2.5 Temporal Attention Pattern

**Paper (Section 3.4):**
> "Second, we find that only a relatively small number of temporal layers are needed and only use temporal attention once every 4 layers"

**Implementation (`dynamics.py` lines 329-332):**
```python
for i in range(num_layers):
    # Temporal attention every temporal_every layers (on the last of each group)
    is_temporal = (i % temporal_every == temporal_every - 1)
    attn_type = "temporal" if is_temporal else "spatial"
```

**Assessment:** CORRECT IMPLEMENTATION

The implementation correctly applies temporal attention every 4th layer as specified in the paper.

#### 2.6 AdaLN Modulation

**Paper (Implied by diffusion transformer design):**
AdaLN is the standard method for timestep conditioning in diffusion transformers.

**Implementation (`dynamics.py` lines 214-255):**
```python
# AdaLN modulation for timestep conditioning
self.adaLN_modulation = nn.Sequential(
    nn.SiLU(),
    nn.Linear(dim, dim * 6),
)

# In forward:
shift1, scale1, gate1, shift2, scale2, gate2 = modulation.chunk(6, dim=-1)
h = self.norm1(x)
h = h * (1 + scale1) + shift1
h = self.attn(h)
x = x + gate1 * h
```

**Assessment:** CORRECT IMPLEMENTATION

The 6-parameter AdaLN modulation (shift, scale, gate for both attention and FFN) is correctly implemented.

---

## 3. Diffusion Training (Paper Section 2, 3.2)

### Paper Specification

| Component | Paper Specification |
|-----------|-------------------|
| Objective | Flow matching with shortcut forcing |
| Prediction | X-prediction (predicts clean data) |
| Loss Weight | Ramp weight: w(tau) = 0.9*tau + 0.1 |
| Shortcut | Bootstrap loss for d > d_min |
| Sampling | K=4 steps at inference |

### Implementation Status

| Component | Implementation | Match |
|-----------|---------------|-------|
| Flow Matching | Linear interpolation | YES |
| X-prediction | Implemented | YES |
| Ramp Weight | Implemented | YES |
| Shortcut Forcing | Skeleton implemented | PARTIAL |
| Diffusion Forcing | Per-timestep tau | YES |

### Detailed Analysis

#### 3.1 X-Prediction

**Paper (Section 3.2):**
> "we found that parameterizing the network to predict clean representations, called x-prediction, enables high-quality rollouts of arbitrary length"

**Implementation (`diffusion.py` lines 212-249):**
```python
def x_prediction_loss(pred, target, tau, use_ramp_weight=True, reduce="mean"):
    # MSE loss per element
    mse = F.mse_loss(pred, target, reduction="none")
    # ...
```

**Assessment:** CORRECT IMPLEMENTATION

X-prediction is correctly implemented.

#### 3.2 Ramp Weight

**Paper (Equation 8):**
> w(tau) = 0.9*tau + 0.1

**Implementation (`diffusion.py` lines 192-209):**
```python
def ramp_weight(tau: torch.Tensor) -> torch.Tensor:
    return 0.9 * tau + 0.1
```

**Assessment:** CORRECT IMPLEMENTATION

Note: The paper states "tau = 0 corresponds to full noise and tau = 1 to clean data" but the implementation uses the opposite convention (tau=0 clean, tau=1 noise). The ramp weight formula works either way as long as it's consistent.

**POTENTIAL BUG:** The comment in the code says:
> "At tau=0 (clean): w = 0.1, At tau=1 (noise): w = 1.0"

But the paper convention is opposite. Verify training code uses consistent tau convention.

#### 3.3 Shortcut Forcing

**Paper (Section 2, 3.2):**
Shortcut forcing enables K=4 step sampling by training with bootstrap targets.

**Implementation (`diffusion.py` lines 306-440):**
```python
class ShortcutForcing:
    """Shortcut forcing objective for few-step sampling.
    Not implemented in MVP - use standard diffusion first.
    """
```

**Assessment:** PARTIAL IMPLEMENTATION

The shortcut forcing class exists but the comment indicates it's not fully integrated. The implementation includes:
- Step size sampling
- Bootstrap loss computation
- Teacher-student framework

**Recommendation:** Integrate shortcut forcing into training loop for K=4 inference.

---

## 4. Model Sizes (Paper Table 3 / Section 4)

### Paper Specification

| Component | Paper Size |
|-----------|-----------|
| Tokenizer | 400M parameters |
| Dynamics | 1.6B parameters |
| **Total** | **2B parameters** |
| Spatial Tokens | 256 (N_z) |
| Context Length | 192 frames |

### Implementation Sizes

```python
# From tokenizer.py
configs = {
    "tiny": {"latent_dim": 128, "base_channels": 32},   # ~1.5M params
    "small": {"latent_dim": 256, "base_channels": 64},  # ~6M params
    "medium": {"latent_dim": 384, "base_channels": 96}, # ~14M params
    "large": {"latent_dim": 512, "base_channels": 128}, # ~25M params
}

# From dynamics.py
configs = {
    "tiny": {"model_dim": 256, "num_layers": 6, "num_heads": 4},
    "small": {"model_dim": 512, "num_layers": 12, "num_heads": 8},
    "medium": {"model_dim": 768, "num_layers": 18, "num_heads": 12},
    "large": {"model_dim": 512, "num_layers": 24, "num_heads": 8},
}
```

### Size Comparison

| Component | Paper | Implementation (large) | Ratio |
|-----------|-------|----------------------|-------|
| Tokenizer | 400M | ~25M | 16x smaller |
| Dynamics | 1.6B | ~50-100M (estimated) | 16-32x smaller |

**Assessment:** INTENTIONAL SIMPLIFICATION

The implementation is significantly smaller than the paper's full-scale model. This is expected for:
- Initial development and debugging
- Resource-constrained experimentation
- Faster iteration cycles

**Recommendation:** Current sizes are appropriate for MVP. Scale up gradually using paper's architecture patterns.

---

## 5. Summary of Deviations

### Critical (Should Fix)

| Issue | Impact | Effort | Priority |
|-------|--------|--------|----------|
| Missing RoPE | Poor length generalization | Medium | HIGH |
| Missing tanh bottleneck | Unbounded latent space | Low | HIGH |
| Tau convention mismatch | Potential training bug | Low | HIGH |

### Important (Should Add)

| Issue | Impact | Effort | Priority |
|-------|--------|--------|----------|
| Missing QKNorm | Training instability at scale | Low | MEDIUM |
| Missing soft capping | Training instability at scale | Low | MEDIUM |
| Missing MAE training | Lower tokenizer quality | Medium | MEDIUM |
| CNN vs Transformer tokenizer | Different architecture | High | MEDIUM |

### Nice to Have (For Scale)

| Issue | Impact | Effort | Priority |
|-------|--------|--------|----------|
| Missing GQA | Inference efficiency | Medium | LOW |
| Missing register tokens | Temporal consistency | Low | LOW |
| Full shortcut integration | K=4 inference | Medium | LOW |

---

## 6. Recommended Action Items

### Phase 1: Critical Fixes
1. **Add tanh bottleneck** to tokenizer encoder output
2. **Verify tau convention** consistency between diffusion.py and training code
3. **Implement RoPE** for spatial and temporal position encoding

### Phase 2: Stability Improvements
4. **Add QKNorm** to attention layers
5. **Add attention soft capping** (cap value ~50 per Gemma 2)
6. **Implement MAE training** for tokenizer

### Phase 3: Scale-Up Preparation
7. **Add GQA** option to attention layers
8. **Add register tokens** to dynamics model
9. **Consider transformer tokenizer** migration
10. **Integrate shortcut forcing** fully into training

---

## 7. Code Quality Notes

### Positive Observations
- Clean, well-documented code
- Correct factorized attention pattern
- Proper AdaLN implementation
- Good separation of concerns (tokenizer, dynamics, diffusion)
- Helpful docstrings and comments

### Areas for Improvement
- Missing type hints in some functions
- Some hardcoded values that could be configurable
- Test coverage not evaluated (separate concern)

---

*This review is based on the DreamerV4 paper arXiv:2509.24527v1 and the implementation files as of the review date.*
